---
title: "Human Activity Recognition"
author: "Pavel Andrienko"
date: "Friday, May 22, 2015"
output:
    html_document:
        pandoc_args: [
          "+RTS", "-K64m",
          "-RTS"
        ]
---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

*The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.*

## Data description

The outcome variable is classe, a factor variable with 5 levels. For this data set, participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:

* exactly according to the specification (Class A)
* throwing the elbows to the front (Class B)
* lifting the dumbbell only halfway (Class C)
* lowering the dumbbell only halfway (Class D)
* throwing the hips to the front (Class E)

## Obtaining Data

```{r}
library(caret)
if(!file.exists("./data")){dir.create("./data")}
if(!file.exists("./storage")){dir.create("./storage")}

# training data
trainingFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingFileName <- "./data/pml-training.csv"
if(!file.exists(trainingFileName)){
    download.file(trainingFileUrl, trainingFileName)
}
trainingData <- read.csv(trainingFileName)
```

## Data processing

```{r}
dim(trainingData)
```

We had 160 variables.

```{r,results='hide'}
summary(trainingData)
```

The output is huge, but we can see that only part of variable contain full data and no NA, and these variables can be splitted in several groups.

Lets make list with names of these variables.

```{r}
type <- c("belt", "arm", "dumbbell", "forearm")
prefix0 <- c("roll", "pitch", "yaw", "total_accel")
prefix1 <- c("gyros", "accel", "magnet")
suffix <- c("x", "y", "z")
cols1 <- as.vector(outer(prefix0, type, paste, sep="_"))
a <- as.vector(outer(prefix1, type, paste, sep="_"))
cols2 <- as.vector(outer(a, suffix, paste, sep="_"))
cols <- c("classe", cols1,cols2)
```

Ok, these a list which we can use to get tidy data from our raw data.

```{r}
trainDS <- trainingData[,cols]
cols
```

## Cross-validation

```{r}
##Create a random partitioning using the caret package
inTrain = createDataPartition(trainDS$classe, p = 3/4)[[1]]
## create a training dataset with 75% random data from the original dataset to build a suitable model
training <- trainDS[inTrain,]
## Create a testing dataset with 25% random data from the original dataset to test the model
testing <- trainDS[-inTrain,]
```

## Model Building

In this section a decision tree and random forest will be applied to the data.

### Decision tree

```{r}
modelDTFilename <- "./storage/modelRpart.rda"
if(!file.exists(modelDTFilename)){
    # Fit model
    modelRpart <- train(classe ~., method="rpart", data=training)
    save(modelRpart, file=modelDTFilename)
} else {
    load(file=modelDTFilename)
}

# Perform prediction
predictDT <- predict(modelRpart, testing)

# Following confusion matrix shows the errors of the prediction algorithm.
confusionMatrix(predictDT, testing$classe)
```

### Random Forest

```{r}
modelRFFilename <- "./storage/modelRF.rda"
if(!file.exists(modelRFFilename)){
    # Fit model
    modelRF <- train(classe ~., method="rf", data=training)
    save(modelRF, file=modelRFFilename)
} else {
    load(file=modelRFFilename)
}

# Perform prediction
predictRF <- predict(modelRF, testing)

# Following confusion matrix shows the errors of the prediction algorithm.
confusionMatrix(predictRF, testing$classe)
```

## Conclusion

### Result

The confusion matrices show, that the Random Forest algorithm performens better than decision trees. The accuracy for the Random Forest model was 1.0 (95% CI : (0.9992, 1)) compared to 0.5037 (95% CI : (0.4896, 0.5178)) for Decision Tree model. The random Forest model is choosen.

### Expected out-of-sample error

The expected out-of-sample error is estimated at 0.0. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. With an accuracy about 100% on our cross-validation data, we can expect that very few, or none, of the test samples will be missclassified.

## Submission

In this section the files for the project submission are generated using the random forest algorithm on the testing data.

```{r}
# testing data
testingFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testingFileName <- "./data/pml-testing.csv"
if(!file.exists(testingFileName)){
    download.file(testingFileUrl, testingFileName)
}
testingData <- read.csv(testingFileName)
testingAssignment <- testingData[,cols[-1]]

modelRFResult <- predict(modelRF, newdata=testingAssignment)

pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("assignment/problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

if(!file.exists("./assignment")){dir.create("./assignment")}
pml_write_files(modelRFResult)
```

According to assignment page 20 of 20 values are predicted correctly.